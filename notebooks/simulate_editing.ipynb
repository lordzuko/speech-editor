{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffb35e8-3509-435c-a051-223ca7049c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/tc046/tc046/lordzuko/miniconda3/envs/fs2/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "\n",
    "MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "FILE_ROOT = os.path.dirname(os.path.realpath(\".\"))\n",
    "PROJECT_ROOT = os.path.dirname(os.path.realpath(\".\"))\n",
    "FILE_ROOT = os.path.join(FILE_ROOT, \"tmp\")\n",
    "os.makedirs(FILE_ROOT, exist_ok=True)\n",
    "\n",
    "os.environ['PYTHONPATH'] = PROJECT_ROOT\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from daft_exprt.extract_features import extract_energy, extract_pitch, mel_spectrogram_HiFi, rescale_wav_to_float32\n",
    "from daft_exprt.hparams import HyperParams\n",
    "from daft_exprt.model import DaftExprt\n",
    "from daft_exprt.cleaners import collapse_whitespace, text_cleaner\n",
    "from daft_exprt.symbols import ascii, eos, punctuation, whitespace\n",
    "from daft_exprt.utils import chunker\n",
    "\n",
    "from hifi_gan.models import Generator\n",
    "from hifi_gan import AttrDict\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0690531-1e82-4938-864a-9204f6c07f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft_exprt.synthesize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6c87ba-6119-445c-a6aa-905b2a8252b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_path = \"/work/tc046/tc046/lordzuko/work/daft-exprt/trainings/daft_bc2013_v1/checkpoints/DaftExprt_best\"\n",
    "vocoder_config_path = \"/work/tc046/tc046/lordzuko/work/daft-exprt/hifi_gan/config_v1.json\"\n",
    "vocoder_chkpt_path = \"/work/tc046/tc046/lordzuko/work/daft-exprt/trainings/hifigan/checkpoints/g_00100000\"\n",
    "daft_config_path = \"/work/tc046/tc046/lordzuko/work/speech-editor/conf/daft_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80a3323-50ab-4fd7-9732-fe6ff0101ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hparams = HyperParams(**json.load(open(daft_config_path)), verbose=False)\n",
    "random.seed(hparams.seed)\n",
    "torch.manual_seed(hparams.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "_logger.warning('You have chosen to seed training. This will turn on the CUDNN deterministic setting, '\n",
    "                'which can slow down your training considerably! You may see unexpected behavior when '\n",
    "                'restarting from checkpoints.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9548ee-fa7c-4914-8a5a-fd7fd2b97f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "model = get_model(chkpt_path, hparams)\n",
    "vocoder = get_vocoder(vocoder_config_path, vocoder_chkpt_path)\n",
    "dictionary = get_dictionary(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c0ca305c-6030-49be-b02a-76a9ed83ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonemize_sentence(sentence, dictionary, hparams):\n",
    "    ''' Phonemize sentence using MFA\n",
    "    '''\n",
    "    # get MFA variables\n",
    "    word_trans = dictionary\n",
    "    g2p_model = hparams.mfa_g2p_model\n",
    "\n",
    "    # characters to consider in the sentence\n",
    "    if hparams.language == 'english':\n",
    "        all_chars = ascii + punctuation\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # clean sentence\n",
    "    # \"that's, an 'example! ' of a sentence. '\"\n",
    "    sentence = text_cleaner(sentence.strip(), hparams.language).lower().strip()\n",
    "    # split sentence:\n",
    "    # [',', \"that's\", ',', 'an', \"example'\", '!', \"'\", 'of', 'a', 'sentence', '.', '.', '.', \"'\"]\n",
    "    sent_words = re.findall(f\"[\\w']+|[{punctuation}]\", sentence.lower().strip())\n",
    "    # remove characters that are not letters or punctuation:\n",
    "    # [',', \"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence', '.', '.', '.']\n",
    "    sent_words = [x for x in sent_words if len(re.sub(f'[^{all_chars}]', '', x)) != 0]\n",
    "    # be sure to begin the sentence with a word and not a punctuation\n",
    "    # [\"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence', '.', '.', '.']\n",
    "    while sent_words[0] in punctuation:\n",
    "        sent_words.pop(0)\n",
    "    # keep only one punctuation type at the end\n",
    "    # [\"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence']\n",
    "    punctuation_end = \".\"\n",
    "    while sent_words[-1] in punctuation:\n",
    "        punctuation_end = sent_words.pop(-1)\n",
    "    sent_words.append(punctuation_end)\n",
    "    \n",
    "    _words = deepcopy(sent_words)\n",
    "\n",
    "    # phonemize words and add word boundaries\n",
    "    sentence_phonemized, unk_words = [], []\n",
    "    while len(sent_words) != 0:\n",
    "        word = sent_words.pop(0)\n",
    "        if word in word_trans:\n",
    "            # phones = random.choice(word_trans[word])\n",
    "            phones = word_trans[word][0]\n",
    "            sentence_phonemized.append(phones)\n",
    "        else:\n",
    "            unk_words.append(word)\n",
    "            sentence_phonemized.append('<unk>')\n",
    "        # at this point we pass to the next word\n",
    "        # we must add a word boundary between two consecutive words\n",
    "        # print(\"sent_words: \", sent_words)\n",
    "        if len(sent_words) != 0:\n",
    "            word_bound = sent_words.pop(0) if sent_words[0] in punctuation else whitespace\n",
    "            sentence_phonemized.append(word_bound)\n",
    "    # add EOS token\n",
    "    sentence_phonemized.append(eos)\n",
    "    \n",
    "    # use MFA g2p model to phonemize unknown words\n",
    "    if len(unk_words) != 0:\n",
    "        rand_name = str(uuid.uuid4())\n",
    "        oovs = os.path.join(FILE_ROOT, f'{rand_name}_oovs.txt')\n",
    "        with open(oovs, 'w', encoding='utf-8') as f:\n",
    "            for word in unk_words:\n",
    "                f.write(f'{word}\\n')\n",
    "        # generate transcription for unknown words\n",
    "        oovs_trans = os.path.join(FILE_ROOT, f'{rand_name}_oovs_trans.txt')\n",
    "        tmp_dir = os.path.join(FILE_ROOT, f'{rand_name}')\n",
    "        os.system(f'mfa g2p {g2p_model} {oovs} {oovs_trans} -t {tmp_dir}')\n",
    "        # extract transcriptions\n",
    "        with open(oovs_trans, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip().split() for line in f.readlines()]\n",
    "        for line in lines:\n",
    "            transcription = line[1:]\n",
    "            unk_idx = sentence_phonemized.index('<unk>')\n",
    "            sentence_phonemized[unk_idx] = transcription\n",
    "        # remove files\n",
    "        os.remove(oovs)\n",
    "        os.remove(oovs_trans)\n",
    "        rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "    nb_symbols = 0\n",
    "    word_idx = 0\n",
    "    idxs = []\n",
    "    words = []\n",
    "    phones = []\n",
    "    ignore_idxs = []\n",
    "    for item in sentence_phonemized:\n",
    "        if isinstance(item, list):  # correspond to phonemes of a word\n",
    "            nb_symbols += len(item)\n",
    "            idxs.append(nb_symbols)\n",
    "            words.append(_words[word_idx])\n",
    "            phones.extend(item)\n",
    "            word_idx += 1\n",
    "        else:  # correspond to word boundaries\n",
    "            nb_symbols += 1\n",
    "            idxs.append(nb_symbols)\n",
    "            words.append(item)\n",
    "            phones.append(item)\n",
    "            ignore_idxs.append(nb_symbols)\n",
    "\n",
    "    return sentence_phonemized, words, phones, idxs, ignore_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4ce653ba-9463-4faf-b536-c2a440fa3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"She wondered why he should be miserable.\"\n",
    "sentences = [text]\n",
    "phonemeized_sents = prepare_sentences_for_inference(sentences,dictionary, hparams)\n",
    "filenames = [\"a.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "16517918-ed67-4e00-b6a4-1253f6c7aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phonemized_sents:  [['SH', 'IY1'], ' ', ['W', 'AH1', 'N', 'D', 'ER0', 'D'], ' ', ['W', 'AY1'], ' ', ['HH', 'IY1'], ' ', ['SH', 'UH1', 'D'], ' ', ['B', 'IY1'], ' ', ['M', 'IH1', 'Z', 'R', 'AH0', 'B', 'AH0', 'L'], '.', '~'] 15\n",
      "words:  ['she', ' ', 'wondered', ' ', 'why', ' ', 'he', ' ', 'should', ' ', 'be', ' ', 'miserable', '.', '~'] 15\n",
      "phones:  ['SH', 'IY1', ' ', 'W', 'AH1', 'N', 'D', 'ER0', 'D', ' ', 'W', 'AY1', ' ', 'HH', 'IY1', ' ', 'SH', 'UH1', 'D', ' ', 'B', 'IY1', ' ', 'M', 'IH1', 'Z', 'R', 'AH0', 'B', 'AH0', 'L', '.', '~'] 33\n",
      "idxs:  [2, 3, 9, 10, 12, 13, 15, 16, 19, 20, 22, 23, 31, 32, 33] 15\n",
      "ignore_idxs:  [3, 10, 13, 16, 20, 23, 32, 33] 8\n"
     ]
    }
   ],
   "source": [
    "# sentence_phonemized, words, phones, idxs, ignore_idxs\n",
    "print(\"phonemized_sents: \", phonemeized_sents[0][0], len(phonemeized_sents[0][0]))\n",
    "print(\"words: \",phonemeized_sents[0][1], len(phonemeized_sents[0][1]))\n",
    "print(\"phones: \", phonemeized_sents[0][2], len(phonemeized_sents[0][2]))\n",
    "print(\"idxs: \", phonemeized_sents[0][3], len(phonemeized_sents[0][3]))\n",
    "print(\"ignore_idxs: \", phonemeized_sents[0][4], len(phonemeized_sents[0][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec704f8-0750-40ab-a152-54306c99e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/tc046/tc046/lordzuko/miniconda3/envs/fs2/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "phonemeized_sents = [phonemeized_sents[0][0]]\n",
    "style_bank = os.path.join(PROJECT_ROOT, 'scripts', 'style_bank', 'english')\n",
    "# ref_path = \"/scratch/space1/tc046/lordzuko/work/data/raw_data/BC2013_daft_orig/CB/wavs/CB-EM-01-05.wav\"\n",
    "ref_path = \"/scratch/space1/tc046/lordzuko/work/data/raw_data/BC2013_daft_orig/CB/wavs/CB-EM-04-96.wav\"\n",
    "# ref_path = \"/scratch/space1/tc046/lordzuko/work/data/raw_data/BC2013_daft_orig/CB/wavs/CB-EM-04-100.wav\"\n",
    "ref_parameters = extract_reference_parameters(ref_path, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a87640-1135-4262-99a2-40c7bd420599",
   "metadata": {},
   "source": [
    "### Initial Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4aefd739-88d2-4d07-8cce-267ca55b8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num symbols:  47\n"
     ]
    }
   ],
   "source": [
    "dur_factor = None #1.25  # decrease speed\n",
    "pitch_transform = 'add'  # pitch shift\n",
    "pitch_factor = None # 50Hz\n",
    "energy_factor = None\n",
    "\n",
    "# add duration factors for each symbol in the sentence\n",
    "dur_factors = [] if dur_factor is not None else None\n",
    "energy_factors = [] if energy_factor is not None else None\n",
    "pitch_factors = [pitch_transform, []] if pitch_factor is not None else None\n",
    "\n",
    "for sentence in phonemeized_sents:\n",
    "    # count number of symbols in the sentence\n",
    "    nb_symbols = 0\n",
    "    for item in sentence:\n",
    "        if isinstance(item, list):  # correspond to phonemes of a word\n",
    "            nb_symbols += len(item)\n",
    "        else:  # correspond to word boundaries\n",
    "            nb_symbols += 1\n",
    "    print(\"num symbols: \", nb_symbols)\n",
    "    # append to lists\n",
    "    if dur_factors is not None:\n",
    "        dur_factors.append([dur_factor for _ in range(nb_symbols)])\n",
    "    if energy_factors is not None:\n",
    "        energy_factors.append([energy_factor for _ in range(nb_symbols)])\n",
    "    if pitch_factors is not None:\n",
    "        pitch_factors[1].append([pitch_factor for _ in range(nb_symbols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "371a6302-a013-4bdb-a11b-12b554130004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs:before-:  tensor([[[-0.0109,  0.0335, -0.7462,  ..., -0.3758,  0.0136, -0.0476],\n",
      "         [-0.0145, -0.1048,  0.1548,  ..., -0.3636,  0.0388, -0.0189],\n",
      "         [ 0.0066,  0.2483,  0.6379,  ..., -0.3526,  0.0311, -0.0394],\n",
      "         ...,\n",
      "         [-0.0240, -0.8161, -0.3820,  ..., -0.3446,  0.0325, -0.0781],\n",
      "         [ 0.1019, -0.0201,  0.0670,  ..., -0.3558,  0.0062, -0.0278],\n",
      "         [-0.1175, -0.2773, -0.1465,  ..., -0.3587,  0.0020, -0.0246]]])\n",
      "shapes:  torch.Size([1, 47]) torch.Size([1, 47]) torch.Size([1, 47])\n",
      "enc_outputs:after-:  tensor([[[-0.0109,  0.0335, -0.7462,  ..., -0.3758,  0.0136, -0.0476],\n",
      "         [-0.0145, -0.1048,  0.1548,  ..., -0.3636,  0.0388, -0.0189],\n",
      "         [ 0.0066,  0.2483,  0.6379,  ..., -0.3526,  0.0311, -0.0394],\n",
      "         ...,\n",
      "         [-0.0240, -0.8161, -0.3820,  ..., -0.3446,  0.0325, -0.0781],\n",
      "         [ 0.1019, -0.0201,  0.0670,  ..., -0.3558,  0.0062, -0.0278],\n",
      "         [-0.1175, -0.2773, -0.1465,  ..., -0.3587,  0.0020, -0.0246]]])\n",
      "no-fc:dp:-before  tensor([[0.0715, 0.0284, 0.0496, 0.0861, 0.0577, 0.0371, 0.0053, 0.0606, 0.0334,\n",
      "         0.0454, 0.0572, 0.0048, 0.0402, 0.0263, 0.0601, 0.0049, 0.0827, 0.0746,\n",
      "         0.0528, 0.0519, 0.0049, 0.0720, 0.0601, 0.0523, 0.0051, 0.0518, 0.0049,\n",
      "         0.0747, 0.0549, 0.0731, 0.0179, 0.0358, 0.0706, 0.0050, 0.0470, 0.0626,\n",
      "         0.0688, 0.0841, 0.0399, 0.0387, 0.0414, 0.0517, 0.0616, 0.0568, 0.0726,\n",
      "         0.0048, 0.0043]]) torch.Size([1, 47])\n",
      "no-fc:dp:  tensor([[0.0715, 0.0284, 0.0496, 0.0861, 0.0577, 0.0371, 0.0000, 0.0606, 0.0334,\n",
      "         0.0454, 0.0572, 0.0000, 0.0402, 0.0263, 0.0601, 0.0000, 0.0827, 0.0746,\n",
      "         0.0528, 0.0519, 0.0000, 0.0720, 0.0601, 0.0523, 0.0000, 0.0518, 0.0000,\n",
      "         0.0747, 0.0549, 0.0731, 0.0000, 0.0358, 0.0706, 0.0000, 0.0470, 0.0626,\n",
      "         0.0688, 0.0841, 0.0399, 0.0387, 0.0414, 0.0517, 0.0616, 0.0568, 0.0726,\n",
      "         0.0000, 0.0000]]) torch.Size([1, 47])\n",
      "no-fc:di:  tensor([[7, 2, 4, 8, 5, 3, 0, 5, 3, 4, 5, 0, 3, 3, 5, 0, 7, 6, 5, 4, 0, 7, 5, 4,\n",
      "         0, 5, 0, 6, 5, 6, 0, 3, 6, 0, 4, 6, 6, 7, 3, 4, 3, 5, 5, 5, 6, 0, 0]]) torch.Size([1, 47])\n"
     ]
    }
   ],
   "source": [
    "speaker_ids = [0]*len(sentences)\n",
    "refs = [ref_parameters]\n",
    "batch_size = 1\n",
    "# generate mel-specs and synthesize audios with Griffin-Lim\n",
    "batch_predictions = generate_mel_specs(model, phonemeized_sents, speaker_ids, refs,\n",
    "                   hparams, dur_factors, energy_factors, pitch_factors, batch_size, filenames)\n",
    "\n",
    "\n",
    "# duration_pred, duration_int, energy_pred, pitch_pred    \n",
    "control_values_init = {}\n",
    "v = batch_predictions[\"a.wav\"]\n",
    "\n",
    "control_values_init[\"d\"] = v[0].unsqueeze(0).detach().cpu().numpy()\n",
    "control_values_init[\"e\"] = v[2].unsqueeze(0).detach().cpu().numpy()\n",
    "control_values_init[\"p\"] = v[3].unsqueeze(0).detach().cpu().numpy()\n",
    "mels = v[4].unsqueeze(0)\n",
    "wavs = vocoder_infer(mels, vocoder, lengths=None)\n",
    "    \n",
    "wavfile.write(os.path.join(\"./\", \"{}\".format(\"a.wav\")), hparams.sampling_rate, wavs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2bdea7f-0f31-44e0-868d-7efcbd24f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07153996, 0.02839828, 0.04961199, 0.08612181, 0.05773258,\n",
       "        0.03707456, 0.        , 0.06056041, 0.03342576, 0.04543884,\n",
       "        0.05716084, 0.        , 0.04015773, 0.02630617, 0.06014895,\n",
       "        0.        , 0.0827309 , 0.07458448, 0.05278093, 0.05188501,\n",
       "        0.        , 0.07195716, 0.06014519, 0.05228197, 0.        ,\n",
       "        0.0517866 , 0.        , 0.07471395, 0.05485927, 0.07311358,\n",
       "        0.        , 0.03576265, 0.07061581, 0.        , 0.04703812,\n",
       "        0.06260144, 0.06879132, 0.08413483, 0.03990091, 0.03874783,\n",
       "        0.04140083, 0.05174303, 0.06163193, 0.05684355, 0.07259117,\n",
       "        0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_values_init[\"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "522e5926-fc74-4754-85f8-467f3531650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.259926  ,  0.0547027 ,  2.2075067 , -0.53379595,  1.4796746 ,\n",
       "        -0.1130432 ,  0.        , -1.1543552 ,  1.0563328 ,  1.0694501 ,\n",
       "        -0.74712014,  0.        , -0.94322866,  0.89613485, -0.63331246,\n",
       "         0.        , -1.130187  ,  0.7843849 , -0.6115345 ,  0.36612192,\n",
       "         0.        , -0.09563295,  0.44997376, -0.64130163,  0.        ,\n",
       "         0.8105938 ,  0.        , -0.58725715,  0.66506827, -1.0940711 ,\n",
       "         0.        ,  0.87069833, -0.8133275 ,  0.        ,  0.6626423 ,\n",
       "         0.9503517 ,  0.6938566 , -1.172732  , -0.3553613 ,  0.49991325,\n",
       "         0.7131107 ,  0.5386547 , -0.7668078 ,  0.04634795,  0.05067344,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_values_init[\"e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb14d402-50f6-417e-bbc0-8695fce99183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02353824,  1.7063059 ,  2.5300465 ,  1.2209028 ,  0.96856123,\n",
       "         0.18960619,  0.        ,  0.0254237 ,  0.99284846,  0.3658704 ,\n",
       "         0.09693921,  0.        ,  0.03084239,  0.27619335,  0.29360187,\n",
       "         0.        ,  0.3773249 , -0.13530944, -0.54148346, -0.12286822,\n",
       "         0.        ,  0.06538778, -0.2298479 , -0.15782307,  0.        ,\n",
       "         0.44838503,  0.        ,  0.11425585,  0.4341223 , -0.1507506 ,\n",
       "         0.        ,  0.7297614 ,  0.4344019 ,  0.        ,  1.0624006 ,\n",
       "         0.9117279 ,  1.0594965 ,  0.42424563,  0.8624264 ,  0.626301  ,\n",
       "         0.6383288 ,  0.6641659 ,  0.27507088,  0.6182978 ,  0.08646534,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_values_init[\"p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f361aa7-d011-4e15-96dd-442a0e7d48ba",
   "metadata": {},
   "source": [
    "### Fine control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a9a65bd-2358-47b8-ab04-55e0c4c7dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc = {}\n",
    "# fc[\"d\"] = np.ones(control_values_init[\"d\"].shape)\n",
    "# fc[\"e\"] = np.ones(control_values_init[\"e\"].shape)\n",
    "# fc[\"p\"] = np.ones(control_values_init[\"p\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2ed159e-6660-42d5-9b5b-c1fb275af2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = {}\n",
    "fc[\"d\"] = control_values_init[\"d\"]\n",
    "# fc[\"d\"][0][:6] *= 2.\n",
    "fc[\"e\"] = control_values_init[\"e\"]\n",
    "fc[\"p\"] = control_values_init[\"p\"]\n",
    "fc[\"p\"][0][:6] *= 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ea3d3b2-4839-4cfc-967c-4879262d0f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07061473,  5.1189175 ,  7.5901394 ,  3.6627083 ,  2.9056838 ,\n",
       "        0.56881857], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc[\"p\"][0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a390c008-fdc4-429d-bce9-dde22a57cf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07061473,  5.1189175 ,  7.5901394 ,  3.6627083 ,  2.9056838 ,\n",
       "         0.56881857,  0.        ,  0.0254237 ,  0.99284846,  0.3658704 ,\n",
       "         0.09693921,  0.        ,  0.03084239,  0.27619335,  0.29360187,\n",
       "         0.        ,  0.3773249 , -0.13530944, -0.54148346, -0.12286822,\n",
       "         0.        ,  0.06538778, -0.2298479 , -0.15782307,  0.        ,\n",
       "         0.44838503,  0.        ,  0.11425585,  0.4341223 , -0.1507506 ,\n",
       "         0.        ,  0.7297614 ,  0.4344019 ,  0.        ,  1.0624006 ,\n",
       "         0.9117279 ,  1.0594965 ,  0.42424563,  0.8624264 ,  0.626301  ,\n",
       "         0.6383288 ,  0.6641659 ,  0.27507088,  0.6182978 ,  0.08646534,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_values_init[\"p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c29c1d3-add1-4118-b860-f02faeae82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_factor = fc[\"d\"] #1.25  # decrease speed\n",
    "pitch_transform = 'add'  # pitch shift\n",
    "pitch_factor = fc[\"p\"] # 50Hz\n",
    "energy_factor = fc[\"e\"]\n",
    "\n",
    "# add duration factors for each symbol in the sentence\n",
    "dur_factors = [] if dur_factor is not None else None\n",
    "energy_factors = [] if energy_factor is not None else None\n",
    "pitch_factors = [pitch_transform, []] if pitch_factor is not None else None\n",
    "\n",
    "if dur_factors is not None:\n",
    "    dur_factors = dur_factor\n",
    "if energy_factors is not None:\n",
    "    energy_factors = energy_factor\n",
    "if pitch_factors is not None:\n",
    "    pitch_factors[1] = pitch_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "39021bf2-f09b-4791-8056-055adc102a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs:before-:  tensor([[[-0.0109,  0.0335, -0.7462,  ..., -0.3758,  0.0136, -0.0476],\n",
      "         [-0.0145, -0.1048,  0.1548,  ..., -0.3636,  0.0388, -0.0189],\n",
      "         [ 0.0066,  0.2483,  0.6379,  ..., -0.3526,  0.0311, -0.0394],\n",
      "         ...,\n",
      "         [-0.0240, -0.8161, -0.3820,  ..., -0.3446,  0.0325, -0.0781],\n",
      "         [ 0.1019, -0.0201,  0.0670,  ..., -0.3558,  0.0062, -0.0278],\n",
      "         [-0.1175, -0.2773, -0.1465,  ..., -0.3587,  0.0020, -0.0246]]])\n",
      "shapes-:  torch.Size([1, 47]) torch.Size([1, 47]) torch.Size([1, 47])\n",
      "fc:dp:-before  tensor([[0.0715, 0.0284, 0.0496, 0.0861, 0.0577, 0.0371, 0.0000, 0.0606, 0.0334,\n",
      "         0.0454, 0.0572, 0.0000, 0.0402, 0.0263, 0.0601, 0.0000, 0.0827, 0.0746,\n",
      "         0.0528, 0.0519, 0.0000, 0.0720, 0.0601, 0.0523, 0.0000, 0.0518, 0.0000,\n",
      "         0.0747, 0.0549, 0.0731, 0.0000, 0.0358, 0.0706, 0.0000, 0.0470, 0.0626,\n",
      "         0.0688, 0.0841, 0.0399, 0.0387, 0.0414, 0.0517, 0.0616, 0.0568, 0.0726,\n",
      "         0.0000, 0.0000]]) torch.Size([1, 47])\n",
      "fc:dp:-after  tensor([[0.0715, 0.0284, 0.0496, 0.0861, 0.0577, 0.0371, 0.0000, 0.0606, 0.0334,\n",
      "         0.0454, 0.0572, 0.0000, 0.0402, 0.0263, 0.0601, 0.0000, 0.0827, 0.0746,\n",
      "         0.0528, 0.0519, 0.0000, 0.0720, 0.0601, 0.0523, 0.0000, 0.0518, 0.0000,\n",
      "         0.0747, 0.0549, 0.0731, 0.0000, 0.0358, 0.0706, 0.0000, 0.0470, 0.0626,\n",
      "         0.0688, 0.0841, 0.0399, 0.0387, 0.0414, 0.0517, 0.0616, 0.0568, 0.0726,\n",
      "         0.0000, 0.0000]]) torch.Size([1, 47])\n",
      "fc:di:  tensor([[7, 2, 4, 8, 5, 3, 0, 5, 3, 4, 5, 0, 3, 3, 5, 0, 7, 6, 5, 4, 0, 7, 5, 4,\n",
      "         0, 5, 0, 6, 5, 6, 0, 3, 6, 0, 4, 6, 6, 7, 3, 4, 3, 5, 5, 5, 6, 0, 0]]) torch.Size([1, 47])\n"
     ]
    }
   ],
   "source": [
    "speaker_ids = [0]*len(sentences)\n",
    "refs = [ref_parameters]\n",
    "batch_size = 1\n",
    "# generate mel-specs and synthesize audios with Griffin-Lim\n",
    "batch_predictions = generate_mel_specs(model, phonemeized_sents, speaker_ids, refs,\n",
    "                   hparams, dur_factors, energy_factors, pitch_factors, batch_size, filenames, fine_control=True)\n",
    "\n",
    "\n",
    "# duration_pred, duration_int, energy_pred, pitch_pred    \n",
    "control_values_updated_fc = {}\n",
    "v = batch_predictions[\"a.wav\"]\n",
    "\n",
    "control_values_updated_fc[\"d\"] = v[0].unsqueeze(0).detach().cpu().numpy()\n",
    "control_values_updated_fc[\"e\"] = v[2].unsqueeze(0).detach().cpu().numpy()\n",
    "control_values_updated_fc[\"p\"] = v[3].unsqueeze(0).detach().cpu().numpy()\n",
    "mels = v[4].unsqueeze(0)\n",
    "wavs = vocoder_infer(mels, vocoder, lengths=None)\n",
    "    \n",
    "wavfile.write(os.path.join(\"./\", \"{}\".format(\"c.wav\")), hparams.sampling_rate, wavs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75bdd909-7b68-457e-8858-8f9b893a2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_ids = [0]*len(sentences)\n",
    "# refs = [ref_parameters]\n",
    "# batch_size = 1\n",
    "# # generate mel-specs and synthesize audios with Griffin-Lim\n",
    "# batch_predictions = generate_mel_specs(model, phonemeized_sents, speaker_ids, refs,\n",
    "#                    hparams, dur_factors, energy_factors, pitch_factors, batch_size, filenames)\n",
    "\n",
    "\n",
    "# # duration_pred, duration_int, energy_pred, pitch_pred    \n",
    "# control_values_updated = {}\n",
    "# v = batch_predictions[\"a.wav\"]\n",
    "\n",
    "# control_values_updated[\"d\"] = v[0].unsqueeze(0).detach().cpu().numpy()\n",
    "# control_values_updated[\"e\"] = v[2].unsqueeze(0).detach().cpu().numpy()\n",
    "# control_values_updated[\"p\"] = v[3].unsqueeze(0).detach().cpu().numpy()\n",
    "# mels = v[4].unsqueeze(0)\n",
    "# wavs = vocoder_infer(mels, vocoder, lengths=None)\n",
    "    \n",
    "# wavfile.write(os.path.join(\"./\", \"{}\".format(\"b.wav\")), hparams.sampling_rate, wavs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3606f3d9-8684-4cb0-84b3-0a066ec329e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_values_updated[\"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cae47662-98ef-4ba8-90af-993b48546f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_values_updated[\"e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81ea6ab9-9447-4ec5-a541-b73567d8f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_values_updated[\"p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898ddbc-a4da-4f9b-9052-1704a33ba6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4329e57f-67bf-447b-aa7c-09219a02d515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([['HH', 'EH1', 'R', 'IY0', 'AH0', 'T'], \n",
    "  ' ', ['S', 'M', 'AY1', 'L', 'D'], ' ', \n",
    "  ['AH0', 'G', 'EY1', 'N'], ',', \n",
    "  ['AE1', 'N', 'D'], ' ', ['HH', 'ER0'], ' ', \n",
    "  ['S', 'M', 'AY1', 'L', 'Z'], ' ', ['G', 'R', 'UW1'], \n",
    "  ' ', ['S', 'T', 'R', 'AO1', 'NG', 'ER0'], '.', '~'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a3c99ea0-be00-4b8c-a21c-ef37f5b027c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['harriet', ' ', 'smiled', ' ', 'again', ',', ',', ' ', 'and', ' ', 'her', ' ', 'smiles', ' ', 'grew', '.', '~'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5802233c-440b-484f-bf2d-00c142d34666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['harriet', 'smiled', 'again', ',', 'and', 'her', 'smiles', 'grew', 'stronger', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56228246-07d5-46d4-838b-a703869e6ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
