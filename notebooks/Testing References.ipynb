{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60f90ff-900b-4643-9d90-baa58f9f372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "\n",
    "MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.realpath(\".\"))\n",
    "# PROJECT_ROOT = os.path.dirname(FILE_ROOT)\n",
    "FILE_ROOT = os.path.join(PROJECT_ROOT, \"tmp\")\n",
    "os.makedirs(FILE_ROOT, exist_ok=True)\n",
    "\n",
    "os.environ['PYTHONPATH'] = PROJECT_ROOT\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from daft_exprt.extract_features import extract_energy, extract_pitch, mel_spectrogram_HiFi, rescale_wav_to_float32\n",
    "from daft_exprt.hparams import HyperParams\n",
    "from daft_exprt.model import DaftExprt\n",
    "from daft_exprt.cleaners import collapse_whitespace, text_cleaner\n",
    "from daft_exprt.symbols import ascii, eos, punctuation, whitespace\n",
    "from daft_exprt.utils import chunker\n",
    "\n",
    "from hifi_gan.models import Generator\n",
    "from hifi_gan import AttrDict\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "random.seed(1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672340dc-49cc-4d69-9ca9-6087f730d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Script example that showcases how to generate with Daft-Exprt\n",
    "    using a target sentence, a target speaker, and a target prosody\n",
    "'''\n",
    "\n",
    "def get_model(chkpt_path, hparams):\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        checkpoint_dict = torch.load(chkpt_path, map_location=f'cuda:{0}')\n",
    "    else:\n",
    "        checkpoint_dict = torch.load(chkpt_path, map_location=f'cpu')\n",
    "\n",
    "    # hparams = HyperParams(verbose=False, **checkpoint_dict['config_params'])\n",
    "    # load model\n",
    "    if gpu_available:\n",
    "        torch.cuda.set_device(0)\n",
    "        model = DaftExprt(hparams).cuda(0)\n",
    "    else:\n",
    "        model = DaftExprt(hparams)\n",
    "\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in checkpoint_dict['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)    \n",
    "\n",
    "    # # define cudnn variables\n",
    "    # random.seed(hparams.seed)\n",
    "    # torch.manual_seed(hparams.seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # _logger.warning('You have chosen to seed training. This will turn on the CUDNN deterministic setting, '\n",
    "    #                 'which can slow down your training considerably! You may see unexpected behavior when '\n",
    "    #                 'restarting from checkpoints.\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def vocoder_infer(mels, vocoder, lengths=None):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        wavs = vocoder(mels).squeeze(1)\n",
    "\n",
    "    wavs = (\n",
    "        wavs.cpu().numpy() * MAX_WAV_VALUE\n",
    "    ).astype(\"int16\")\n",
    "    wavs = [wav for wav in wavs]\n",
    "\n",
    "    for i in range(len(mels)):\n",
    "        if lengths is not None:\n",
    "            wavs[i] = wavs[i][: lengths[i]]\n",
    "\n",
    "    return wavs\n",
    "\n",
    "\n",
    "def get_vocoder(config_path, chkpt_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        config = AttrDict(config)\n",
    "\n",
    "    vocoder = Generator(config)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        ckpt = torch.load(chkpt_path)\n",
    "    else:\n",
    "        ckpt = torch.load(chkpt_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    vocoder.load_state_dict(ckpt[\"generator\"])\n",
    "    vocoder.eval()\n",
    "    vocoder.remove_weight_norm()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        vocoder.to(f\"cuda:{0}\")\n",
    "\n",
    "    return vocoder\n",
    "\n",
    "\n",
    "def get_dictionary(hparams):\n",
    "    dictionary = hparams.mfa_dictionary\n",
    "    # load dictionary and extract word transcriptions\n",
    "    word_trans = collections.defaultdict(list)\n",
    "    with open(dictionary, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip().split() for line in f.readlines()] \n",
    "    for line in lines:\n",
    "        word_trans[line[0].lower()].append(line[1:])\n",
    "    return word_trans\n",
    "\n",
    "def phonemize_sentence(sentence, dictionary, hparams):\n",
    "    ''' Phonemize sentence using MFA\n",
    "    '''\n",
    "    # get MFA variables\n",
    "    word_trans = dictionary\n",
    "    g2p_model = hparams.mfa_g2p_model\n",
    "\n",
    "    # characters to consider in the sentence\n",
    "    if hparams.language == 'english':\n",
    "        all_chars = ascii + punctuation\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # clean sentence\n",
    "    # \"that's, an 'example! ' of a sentence. '\"\n",
    "    sentence = text_cleaner(sentence.strip(), hparams.language).lower().strip()\n",
    "    # split sentence:\n",
    "    # [',', \"that's\", ',', 'an', \"example'\", '!', \"'\", 'of', 'a', 'sentence', '.', '.', '.', \"'\"]\n",
    "    sent_words = re.findall(f\"[\\w']+|[{punctuation}]\", sentence.lower().strip())\n",
    "    # remove characters that are not letters or punctuation:\n",
    "    # [',', \"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence', '.', '.', '.']\n",
    "    sent_words = [x for x in sent_words if len(re.sub(f'[^{all_chars}]', '', x)) != 0]\n",
    "    # be sure to begin the sentence with a word and not a punctuation\n",
    "    # [\"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence', '.', '.', '.']\n",
    "    while sent_words[0] in punctuation:\n",
    "        sent_words.pop(0)\n",
    "    # keep only one punctuation type at the end\n",
    "    # [\"that's\", ',', 'an', \"example'\", '!', 'of', 'a', 'sentence']\n",
    "    punctuation_end = \".\"\n",
    "    while sent_words[-1] in punctuation:\n",
    "        punctuation_end = sent_words.pop(-1)\n",
    "    sent_words.append(punctuation_end)\n",
    "    \n",
    "    _words = deepcopy(sent_words)\n",
    "\n",
    "    # phonemize words and add word boundaries\n",
    "    sentence_phonemized, unk_words = [], []\n",
    "    while len(sent_words) != 0:\n",
    "        word = sent_words.pop(0)\n",
    "        if word in word_trans:\n",
    "            phones = random.choice(word_trans[word])\n",
    "            sentence_phonemized.append(phones)\n",
    "        else:\n",
    "            unk_words.append(word)\n",
    "            sentence_phonemized.append('<unk>')\n",
    "        # at this point we pass to the next word\n",
    "        # we must add a word boundary between two consecutive words\n",
    "        print(\"sent_words: \", sentence_phonemized)\n",
    "        if len(sent_words) != 0:\n",
    "            word_bound = sent_words.pop(0) if sent_words[0] in punctuation else whitespace\n",
    "            sentence_phonemized.append(word_bound)\n",
    "    # add EOS token\n",
    "    sentence_phonemized.append(eos)\n",
    "    \n",
    "    # use MFA g2p model to phonemize unknown words\n",
    "    if len(unk_words) != 0:\n",
    "        rand_name = str(uuid.uuid4())\n",
    "        oovs = os.path.join(FILE_ROOT, f'{rand_name}_oovs.txt')\n",
    "        with open(oovs, 'w', encoding='utf-8') as f:\n",
    "            for word in unk_words:\n",
    "                f.write(f'{word}\\n')\n",
    "        # generate transcription for unknown words\n",
    "        oovs_trans = os.path.join(FILE_ROOT, f'{rand_name}_oovs_trans.txt')\n",
    "        tmp_dir = os.path.join(FILE_ROOT, f'{rand_name}')\n",
    "        os.system(f'mfa g2p {g2p_model} {oovs} {oovs_trans} -t {tmp_dir}')\n",
    "        # extract transcriptions\n",
    "        with open(oovs_trans, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip().split() for line in f.readlines()]\n",
    "        for line in lines:\n",
    "            transcription = line[1:]\n",
    "            unk_idx = sentence_phonemized.index('<unk>')\n",
    "            sentence_phonemized[unk_idx] = transcription\n",
    "        # remove files\n",
    "        os.remove(oovs)\n",
    "        os.remove(oovs_trans)\n",
    "        rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "    print(\"-- WORDS -- \", _words)\n",
    "    nb_symbols = 0\n",
    "    word_idx = 0\n",
    "    idxs = []\n",
    "    words = []\n",
    "    phones = []\n",
    "    ignore_idxs = []\n",
    "    for item in sentence_phonemized:\n",
    "        print(\"-- item --: \", item)\n",
    "        if isinstance(item, list):  # correspond to phonemes of a word\n",
    "            nb_symbols += len(item)\n",
    "            idxs.append(nb_symbols)\n",
    "            words.append(_words[word_idx])\n",
    "            phones.extend(item)\n",
    "            word_idx += 1\n",
    "        else:  # correspond to word boundaries\n",
    "            nb_symbols += 1\n",
    "            idxs.append(nb_symbols)\n",
    "            words.append(item)\n",
    "            if item in _words[word_idx:]:\n",
    "                word_idx += 1\n",
    "            phones.append(item)\n",
    "            ignore_idxs.append(nb_symbols)\n",
    "        \n",
    "\n",
    "    print(\"WORDS!!! \", words)\n",
    "\n",
    "\n",
    "    return sentence_phonemized, words, phones, idxs, ignore_idxs\n",
    "\n",
    "\n",
    "def prepare_sentences_for_inference(sentences, dictionary, hparams):\n",
    "    \"\"\"\n",
    "    Phonemize and format sentences to synthesize\n",
    "    \"\"\"\n",
    "    phonemized_sents = []\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    for sent in sentences:\n",
    "        ps, words, phones, idxs, ignore_idxs  = phonemize_sentence(sent, dictionary, hparams)\n",
    "        phonemized_sents.append((ps, words, phones, idxs, ignore_idxs))\n",
    "    return phonemized_sents\n",
    "\n",
    "\n",
    "def extract_reference_parameters(audio_ref, hparams):\n",
    "    ''' Extract energy, pitch and mel-spectrogram parameters from audio\n",
    "        Save numpy arrays to .npz file\n",
    "    '''\n",
    "    # read wav file to range [-1, 1] in np.float32\n",
    "    wav, fs = librosa.load(audio_ref, sr=hparams.sampling_rate)\n",
    "    wav = rescale_wav_to_float32(wav)\n",
    "    # get log pitch\n",
    "    # pitch = extract_pitch(wav, fs, hparams)\n",
    "    pitch = extract_pitch(wav, hparams)\n",
    "    # extract mel-spectrogram\n",
    "    mel_spec = mel_spectrogram_HiFi(wav, hparams)\n",
    "    # get energy\n",
    "    energy = extract_energy(np.exp(mel_spec))\n",
    "    # check sizes are correct\n",
    "    assert(len(pitch) == mel_spec.shape[1]), f'{len(pitch)} -- {mel_spec.shape[1]}'\n",
    "    assert(len(energy) == mel_spec.shape[1]), f'{len(energy)} -- {mel_spec.shape[1]}'\n",
    "\n",
    "    return {\"pitch\": pitch, \"energy\": energy, \"mel_spec\": mel_spec}\n",
    "\n",
    "# generate mel-specs and synthesize audios with Griffin-Lim\n",
    "def generate_mel_specs(model, sentences, speaker_ids, refs,\n",
    "                       hparams, dur_factors, energy_factors, pitch_factors, batch_size, file_names, fine_control=False):\n",
    "    model.eval()\n",
    "    # set default values if prosody factors are None\n",
    "    dur_factors = [None for _ in range(len(sentences))] if dur_factors is None else dur_factors\n",
    "    energy_factors = [None for _ in range(len(sentences))] if energy_factors is None else energy_factors\n",
    "    pitch_factors = ['add', [None for _ in range(len(sentences))]] if pitch_factors is None else pitch_factors\n",
    "    # get pitch transform\n",
    "    pitch_transform = pitch_factors[0].lower()\n",
    "    pitch_factors = pitch_factors[1]\n",
    "    assert(pitch_transform in ['add', 'multiply']), _logger.error(f'Pitch transform \"{pitch_transform}\" is not currently supported')\n",
    "    # check lists have the same size\n",
    "    \n",
    "    assert (len(speaker_ids) == len(sentences)), _logger.error(f'{len(speaker_ids)} speaker IDs but there are {len(sentences)} sentences to generate')\n",
    "    assert (len(refs) == len(sentences)), _logger.error(f'{len(refs)} references but there are {len(sentences)} sentences to generate')\n",
    "    assert (len(dur_factors) == len(sentences)), _logger.error(f'{len(dur_factors)} duration factors but there are {len(sentences)} sentences to generate')\n",
    "    assert (len(energy_factors) == len(sentences)), _logger.error(f'{len(energy_factors)} energy factors but there are {len(sentences)} sentences to generate')\n",
    "    assert (len(pitch_factors) == len(sentences)), _logger.error(f'{len(pitch_factors)} pitch factors but there are {len(sentences)} sentences to generate')\n",
    "    \n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_sentences, batch_refs, batch_dur_factors, batch_energy_factors, \\\n",
    "            batch_pitch_factors, batch_speaker_ids, batch_file_names in \\\n",
    "                zip(chunker(sentences, batch_size), chunker(refs, batch_size), \n",
    "                    chunker(dur_factors, batch_size), chunker(energy_factors, batch_size),\n",
    "                    chunker(pitch_factors, batch_size), chunker(speaker_ids, batch_size),\n",
    "                    chunker(file_names, batch_size)):\n",
    "        \n",
    "            batch_predictions =  generate_batch_mel_specs(\n",
    "                                        model, batch_sentences, batch_refs, batch_dur_factors,\n",
    "                                        batch_energy_factors, batch_pitch_factors, pitch_transform,\n",
    "                                        batch_speaker_ids, batch_file_names, hparams, fine_control=fine_control)\n",
    "\n",
    "            predictions.update(batch_predictions)\n",
    "\n",
    "    return batch_predictions\n",
    "\n",
    "def collate_tensors(batch_sentences, batch_dur_factors, batch_energy_factors,\n",
    "                    batch_pitch_factors, pitch_transform, batch_refs,\n",
    "                    batch_speaker_ids, batch_file_names, hparams):\n",
    "    ''' Extract PyTorch tensors for each sentence and collate them for batch generation\n",
    "    '''\n",
    "    # gather batch\n",
    "    batch = []\n",
    "    for sentence, dur_factors, energy_factors, pitch_factors, refs in \\\n",
    "        zip(batch_sentences, batch_dur_factors, batch_energy_factors, batch_pitch_factors, batch_refs):\n",
    "            # encode input text as a sequence of int symbols\n",
    "            symbols = []\n",
    "            for item in sentence:\n",
    "                if isinstance(item, list):  # correspond to phonemes of a word\n",
    "                    symbols += [hparams.symbols.index(phone) for phone in item]\n",
    "                else:  # correspond to word boundaries\n",
    "                    symbols.append(hparams.symbols.index(item))\n",
    "            symbols = torch.IntTensor(symbols)  # (L, )\n",
    "            # extract duration factors\n",
    "            if dur_factors is None:\n",
    "                dur_factors = [1. for _ in range(len(symbols))]\n",
    "            dur_factors = torch.FloatTensor(dur_factors)  # (L, )\n",
    "            assert(len(dur_factors) == len(symbols)), \\\n",
    "                _logger.error(f'{len(dur_factors)} duration factors whereas there a {len(symbols)} symbols')\n",
    "            # extract energy factors\n",
    "            if energy_factors is None:\n",
    "                energy_factors = [1. for _ in range(len(symbols))]\n",
    "            energy_factors = torch.FloatTensor(energy_factors)  # (L, )\n",
    "            assert(len(energy_factors) == len(symbols)), \\\n",
    "                _logger.error(f'{len(energy_factors)} energy factors whereas there a {len(symbols)} symbols')\n",
    "            # extract pitch factors\n",
    "            if pitch_factors is None:\n",
    "                if pitch_transform == 'add':\n",
    "                    pitch_factors = [0. for _ in range(len(symbols))]\n",
    "                elif pitch_transform == 'multiply':\n",
    "                    pitch_factors = [1. for _ in range(len(symbols))]\n",
    "            pitch_factors = torch.FloatTensor(pitch_factors)  # (L, )\n",
    "            assert(len(pitch_factors) == len(symbols)), \\\n",
    "                _logger.error(f'{len(pitch_factors)} pitch factors whereas there a {len(symbols)} symbols')\n",
    "            # extract references\n",
    "            # refs = np.load(refs)\n",
    "            energy_ref, pitch_ref, mel_spec_ref = refs['energy'], refs['pitch'], refs['mel_spec']\n",
    "            energy_ref = torch.from_numpy(energy_ref).float()  # (T_ref, )\n",
    "            pitch_ref = torch.from_numpy(pitch_ref).float()  # (T_ref, )\n",
    "            mel_spec_ref = torch.from_numpy(mel_spec_ref).float()  # (n_mel_channels, T_ref)\n",
    "            # gather data\n",
    "            batch.append([symbols, dur_factors, energy_factors, pitch_factors, energy_ref, pitch_ref, mel_spec_ref])\n",
    "    \n",
    "    # find symbols sequence max length\n",
    "    input_lengths, ids_sorted_decreasing = \\\n",
    "        torch.sort(torch.LongTensor([len(x[0]) for x in batch]), dim=0, descending=True)\n",
    "    max_input_len = input_lengths[0]\n",
    "    # right pad sequences to max input length\n",
    "    symbols = torch.LongTensor(len(batch), max_input_len).zero_()\n",
    "    dur_factors = 1 + torch.FloatTensor(len(batch), max_input_len).zero_()\n",
    "    energy_factors = 1 + torch.FloatTensor(len(batch), max_input_len).zero_()\n",
    "    if pitch_transform == 'add':\n",
    "        pitch_factors = torch.FloatTensor(len(batch), max_input_len).zero_()\n",
    "    elif pitch_transform == 'multiply':\n",
    "        pitch_factors = 1 + torch.FloatTensor(len(batch), max_input_len).zero_()\n",
    "    \n",
    "    # fill padded arrays\n",
    "    for i in range(len(ids_sorted_decreasing)):\n",
    "        # extract batch sequences\n",
    "        symbols_seq = batch[ids_sorted_decreasing[i]][0]\n",
    "        dur_factors_seq = batch[ids_sorted_decreasing[i]][1]\n",
    "        energy_factors_seq = batch[ids_sorted_decreasing[i]][2]\n",
    "        pitch_factors_seq = batch[ids_sorted_decreasing[i]][3]\n",
    "        # add sequences to padded arrays\n",
    "        symbols[i, :symbols_seq.size(0)] = symbols_seq\n",
    "        dur_factors[i, :dur_factors_seq.size(0)] = dur_factors_seq\n",
    "        energy_factors[i, :energy_factors_seq.size(0)] = energy_factors_seq\n",
    "        pitch_factors[i, :pitch_factors_seq.size(0)] = pitch_factors_seq\n",
    "    \n",
    "    # find reference max length\n",
    "    max_ref_len = max([x[6].size(1) for x in batch])\n",
    "    # right zero-pad references to max output length\n",
    "    energy_refs = torch.FloatTensor(len(batch), max_ref_len).zero_()\n",
    "    pitch_refs = torch.FloatTensor(len(batch), max_ref_len).zero_()\n",
    "    mel_spec_refs = torch.FloatTensor(len(batch), hparams.n_mel_channels, max_ref_len).zero_()\n",
    "    ref_lengths = torch.LongTensor(len(batch))\n",
    "    # fill padded arrays\n",
    "    for i in range(len(ids_sorted_decreasing)):\n",
    "        # extract batch sequences\n",
    "        energy_ref_seq = batch[ids_sorted_decreasing[i]][4]\n",
    "        pitch_ref_seq = batch[ids_sorted_decreasing[i]][5]\n",
    "        mel_spec_ref_seq = batch[ids_sorted_decreasing[i]][6]\n",
    "        # add sequences to padded arrays\n",
    "        energy_refs[i, :energy_ref_seq.size(0)] = energy_ref_seq\n",
    "        pitch_refs[i, :pitch_ref_seq.size(0)] = pitch_ref_seq\n",
    "        mel_spec_refs[i, :, :mel_spec_ref_seq.size(1)] = mel_spec_ref_seq\n",
    "        ref_lengths[i] = mel_spec_ref_seq.size(1)\n",
    "    \n",
    "    # reorganize speaker IDs and file names\n",
    "    file_names = []\n",
    "    speaker_ids = torch.LongTensor(len(batch))\n",
    "    for i in range(len(ids_sorted_decreasing)):\n",
    "        file_names.append(batch_file_names[ids_sorted_decreasing[i]])\n",
    "        speaker_ids[i] = batch_speaker_ids[ids_sorted_decreasing[i]]\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        _logger.info(\"GPU available, Moving Tensors to GPU!\")\n",
    "        symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \\\n",
    "        energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids = to_gpu(\n",
    "            symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \n",
    "           energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids\n",
    "        )\n",
    "\n",
    "    return symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \\\n",
    "        energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids, file_names\n",
    "\n",
    "\n",
    "def to_gpu(symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \n",
    "           energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids):\n",
    "    # put tensors on GPU\n",
    "    gpu = next(model.parameters()).device\n",
    "    symbols = symbols.cuda(gpu, non_blocking=True).long()  # (B, L_max)\n",
    "    dur_factors = dur_factors.cuda(gpu, non_blocking=True).float()  # (B, L_max)\n",
    "    energy_factors = energy_factors.cuda(gpu, non_blocking=True).float()  # (B, L_max)\n",
    "    pitch_factors = pitch_factors.cuda(gpu, non_blocking=True).float()  # (B, L_max)\n",
    "    input_lengths = input_lengths.cuda(gpu, non_blocking=True).long()  # (B, )\n",
    "    energy_refs = energy_refs.cuda(gpu, non_blocking=True).float()  # (B, T_max)\n",
    "    pitch_refs = pitch_refs.cuda(gpu, non_blocking=True).float()  # (B, T_max)\n",
    "    mel_spec_refs = mel_spec_refs.cuda(gpu, non_blocking=True).float()  # (B, n_mel_channels, T_max)\n",
    "    ref_lengths = ref_lengths.cuda(gpu, non_blocking=True).long()  # (B, )\n",
    "    speaker_ids = speaker_ids.cuda(gpu, non_blocking=True).long()  # (B, )\n",
    "\n",
    "    return symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \\\n",
    "        energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids\n",
    "\n",
    "def to_cpu(duration_preds, durations_int, energy_preds, pitch_preds, \n",
    "                        input_lengths, mel_spec_preds, output_lengths, weights):\n",
    "    # transfer data to cpu and convert to numpy array\n",
    "    duration_preds = duration_preds.detach().cpu().numpy()  # (B, L_max)\n",
    "    durations_int = durations_int.detach().cpu().numpy()  # (B, L_max)\n",
    "    energy_preds = energy_preds.detach().cpu().numpy()  # (B, L_max)\n",
    "    pitch_preds = pitch_preds.detach().cpu().numpy()  # (B, L_max)\n",
    "    input_lengths = input_lengths.detach().cpu().numpy()  # (B, )\n",
    "    mel_spec_preds = mel_spec_preds.detach().cpu().numpy()  # (B, n_mel_channels, T_max)\n",
    "    output_lengths = output_lengths.detach().cpu().numpy()  # (B)\n",
    "    weights = weights.detach().cpu().numpy()  # (B, L_max, T_max)\n",
    "\n",
    "    return duration_preds, durations_int, energy_preds, pitch_preds, input_lengths, \\\n",
    "            mel_spec_preds, output_lengths, weights\n",
    "\n",
    "def generate_batch_mel_specs(model, batch_sentences, batch_refs, batch_dur_factors,\n",
    "                                batch_energy_factors, batch_pitch_factors, pitch_transform,\n",
    "                                batch_speaker_ids, batch_file_names, hparams, fine_control=False):\n",
    "\n",
    "    # collate batch tensors\n",
    "    symbols, dur_factors, energy_factors, pitch_factors, input_lengths, \\\n",
    "        energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids, file_names = \\\n",
    "            collate_tensors(batch_sentences, batch_dur_factors, batch_energy_factors,\n",
    "                            batch_pitch_factors, pitch_transform, batch_refs,\n",
    "                            batch_speaker_ids, batch_file_names, hparams)\n",
    "    \n",
    "    # perform inference\n",
    "    inputs = (symbols, dur_factors, energy_factors, pitch_factors, input_lengths,\n",
    "              energy_refs, pitch_refs, mel_spec_refs, ref_lengths, speaker_ids)\n",
    "    \n",
    "\n",
    "    encoder_preds, decoder_preds, alignments = model.inference(inputs, pitch_transform, hparams, fine_control=fine_control)\n",
    "\n",
    "    # parse outputs\n",
    "    duration_preds, durations_int, energy_preds, pitch_preds, input_lengths = encoder_preds\n",
    "    mel_spec_preds, output_lengths = decoder_preds\n",
    "    weights = alignments\n",
    "\n",
    "    # transfer data to cpu and convert to numpy array\n",
    "    # duration_preds, durations_int, energy_preds, pitch_preds, \\\n",
    "    # input_lengths, mel_spec_preds, output_lengths, weights = to_cpu(duration_preds, \n",
    "    #             durations_int, energy_preds, pitch_preds, input_lengths, mel_spec_preds, output_lengths, weights)\n",
    "    \n",
    "    # save preds for each element in the batch\n",
    "    predictions = {}\n",
    "    for line_idx in range(mel_spec_preds.shape[0]):\n",
    "        # crop prosody preds to the correct length\n",
    "        duration_pred = duration_preds[line_idx, :input_lengths[line_idx]]  # (L, )\n",
    "        duration_int = durations_int[line_idx, :input_lengths[line_idx]]  # (L, )\n",
    "        energy_pred = energy_preds[line_idx, :input_lengths[line_idx]]  # (L, )\n",
    "        pitch_pred = pitch_preds[line_idx, :input_lengths[line_idx]]  # (L, )\n",
    "        # crop mel-spec to the correct length\n",
    "        mel_spec_pred = mel_spec_preds[line_idx, :, :output_lengths[line_idx]]  # (n_mel_channels, T)\n",
    "        # crop weights to the correct length\n",
    "        weight = weights[line_idx, :input_lengths[line_idx], :output_lengths[line_idx]]\n",
    "        # save generated spectrogram\n",
    "        file_name = file_names[line_idx]\n",
    "        # store predictions \n",
    "        predictions[f'{file_name}'] = [duration_pred, duration_int, energy_pred, pitch_pred, mel_spec_pred, weight]\n",
    "    return predictions\n",
    "\n",
    "def synthesize(model,vocoder, phonemeized_sents, hparams, ref_path, pitch_factor=None, dur_factor=None, energy_factor=None, fine_control=False):\n",
    "    # style_bank = os.path.join(PROJECT_ROOT, 'scripts', 'style_bank', 'english')\n",
    "    ref_parameters = extract_reference_parameters(ref_path, hparams)\n",
    "\n",
    "    # dur_factor = 1 #1.25  # decrease speed\n",
    "    pitch_transform = 'add'  # pitch shift\n",
    "    # pitch_factor = 0  # 50Hz\n",
    "    # energy_factor = 1\n",
    "    filenames = [\"a.wav\"]\n",
    "    # add duration factors for each symbol in the sentence\n",
    "    dur_factors = [] if dur_factor is not None else None\n",
    "    energy_factors = [] if energy_factor is not None else None\n",
    "    pitch_factors = [pitch_transform, []] if pitch_factor is not None else None\n",
    "\n",
    "    if dur_factors is not None:\n",
    "        dur_factors = dur_factor\n",
    "    if energy_factors is not None:\n",
    "        energy_factors = energy_factor\n",
    "    if pitch_factors is not None:\n",
    "        pitch_factors[1] = pitch_factor\n",
    "\n",
    "    speaker_ids = [0]*len(phonemeized_sents)\n",
    "    refs = [ref_parameters]\n",
    "    batch_size = 1\n",
    "    # generate mel-specs and synthesize audios with Griffin-Lim\n",
    "    batch_predictions = generate_mel_specs(model, phonemeized_sents, speaker_ids, refs,\n",
    "                       hparams, dur_factors, energy_factors, pitch_factors, batch_size, filenames, fine_control=fine_control)\n",
    "\n",
    "    # duration_pred, duration_int, energy_pred, pitch_pred    \n",
    "    control_values = {}\n",
    "    v = batch_predictions[\"a.wav\"]\n",
    "\n",
    "    control_values[\"d\"] = v[0].unsqueeze(0).detach().cpu().numpy()\n",
    "    control_values[\"e\"] = v[2].unsqueeze(0).detach().cpu().numpy()\n",
    "    control_values[\"p\"] = v[3].unsqueeze(0).detach().cpu().numpy()\n",
    "    mels = v[4].unsqueeze(0)\n",
    "    wavs = vocoder_infer(mels, vocoder, lengths=None)\n",
    "    return control_values, wavs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4018be67-a950-4061-b683-65ed1a87184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "chkpt_path = \"/home/lordzuko/work/speech-editor/assets/models/DaftExprt_best\"\n",
    "vocoder_config_path = \"/home/lordzuko/work/speech-editor/conf/vocoder_config.json\"\n",
    "vocoder_chkpt_path = \"/home/lordzuko/work/speech-editor/assets/models/g_00100000\"\n",
    "daft_config_path = \"/home/lordzuko/work/speech-editor/conf/daft_config.json\"\n",
    "hparams = HyperParams(**json.load(open(daft_config_path)), verbose=False)\n",
    "random.seed(hparams.seed)\n",
    "torch.manual_seed(hparams.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "_logger.warning('You have chosen to seed training. This will turn on the CUDNN deterministic setting, '\n",
    "                'which can slow down your training considerably! You may see unexpected behavior when '\n",
    "                'restarting from checkpoints.\\n')\n",
    "\n",
    "model = get_model(chkpt_path, hparams)\n",
    "vocoder = get_vocoder(vocoder_config_path, vocoder_chkpt_path)\n",
    "dictionary = get_dictionary(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecd256-7ef2-44f1-9a2f-057436f60b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae6666-da63-47de-bc4f-290d5dc70355",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize(model,vocoder, phonemeized_sents, hparams, ref_path, pitch_factor=None, dur_factor=None, energy_factor=None, fine_control=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se-daft-env",
   "language": "python",
   "name": "se_daft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
